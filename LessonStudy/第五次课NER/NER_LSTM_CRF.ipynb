{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "MIN_FREQ = 1\n",
    "UNK, PAD = \"<UNK>\", \"<PAD>\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T10:55:28.796477700Z",
     "start_time": "2023-05-16T10:55:28.788500200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def build_vocab(train_path, class_ls_path):  # 构建词典（默认是字符级）\n",
    "    vocab_dic = {}  # 字典\n",
    "    class_set = set()  # 集合\n",
    "    with open(train_path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f):\n",
    "            lin = line.strip()  # 去除头尾的空格 换行符 制表符\n",
    "            if not lin:\n",
    "                continue\n",
    "            content, label = lin.split()\n",
    "            vocab_dic[content] = vocab_dic.get(content, 0) + 1  # 每个字计数\n",
    "            class_set.add(label)\n",
    "        vocab_ls = sorted([_ for _ in vocab_dic.items() if _[1] >= MIN_FREQ], key=lambda x: x[1], reverse=True)[\n",
    "                   :MAX_VOCAB_SIZE]\n",
    "        class_ls = list(sorted(class_set))\n",
    "        with open(class_ls_path, \"w\", encoding='utf-8') as cf:\n",
    "            cf.write('\\n'.join(str(label) for label in class_ls))\n",
    "            cf.write('\\n' + PAD)\n",
    "\n",
    "        vocab_dic = {word_count[0]: idx for idx, word_count in enumerate(vocab_ls)}\n",
    "        vocab_dic.update({UNK: len(vocab_dic), PAD: len(vocab_dic) + 1})  # 将UNK和PAD\n",
    "    return vocab_dic"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T10:55:29.855099400Z",
     "start_time": "2023-05-16T10:55:29.837630300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# vocab_dic, word_pad_id, label_pad_id = build_vocab('ner.train', 'ner.label')  # 传入数据集"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "67344it [00:00, 1378054.50it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab_dic = build_vocab(r'D:\\PycharmProjects\\nlp\\LessonStudy\\第五次课NER\\ner.train', 'ner.label')  # 传入数据集"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T10:55:30.615487200Z",
     "start_time": "2023-05-16T10:55:30.556641600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "573"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dic['专']  # 检验是否运行正常"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T10:55:31.265428200Z",
     "start_time": "2023-05-16T10:55:31.253952600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def make_tensor(tensor, config):\n",
    "    tensor_ret = torch.LongTensor(tensor).to(config.device)\n",
    "    return tensor_ret\n",
    "\n",
    "class Mydataset(Dataset):\n",
    "    def __init__(self, filepath, config, vocab):\n",
    "        self.filepath = filepath\n",
    "        self.vocab = vocab\n",
    "        self.label_dic = self._getLabelDic(config)\n",
    "        self.data_label = self._get_contents(config)\n",
    "        self.x = make_tensor(torch.tensor([_[0] for _ in self.data_label]), config)\n",
    "        self.y = make_tensor(torch.tensor([_[1] for _ in self.data_label]), config)\n",
    "        self.len = len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):  # (x, seq_len)构成一个元组，并返回标签\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def _getLabelDic(self, config):\n",
    "        label_dic = {}\n",
    "        with open(config.class_ls_path, 'r', encoding='utf-8') as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                label = line.strip()\n",
    "                label_dic[label] = idx\n",
    "        return label_dic\n",
    "\n",
    "    def _get_contents(self, config):\n",
    "        contents = []\n",
    "        with open(self.filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in tqdm(f):\n",
    "                lin = line.strip()\n",
    "                if not lin:\n",
    "                    continue\n",
    "                word, label = lin.split()\n",
    "                word_id = self.vocab.get(word, self.vocab.get(UNK))  # dict.get\n",
    "                label_id = self.label_dic.get(label)\n",
    "                contents.append((word_id, label_id))\n",
    "            return contents  # [([...], 0), ([...], 1), ...]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T10:55:31.923982800Z",
     "start_time": "2023-05-16T10:55:31.914009300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def extract_vocab_tensor(config):  # 提取vocab内的预训练词向量\n",
    "    if config.embedding_type == 'random':  # 随机初始化\n",
    "        embedding_pretrained = None\n",
    "    else:  # 加载预训练词向量\n",
    "        vocab_tensor_path = config.pretrain_dir + config.embedding_type\n",
    "        if os.path.exists(vocab_tensor_path):  # 已构建则直接加载\n",
    "            embedding_pretrained = np.load(vocab_tensor_path)['embeddings'].astype('float32')\n",
    "        else:  # 重新构建\n",
    "            with open(config.vocab_path, 'rb') as vocab_f:\n",
    "                word_to_id = pickle.load(vocab_f)\n",
    "                pretrained_f = open(config.pretrain_dir, 'r', encoding='utf-8')\n",
    "                embeddings = np.random.rand(len(word_to_id), config.embedding_dim)\n",
    "                for i, line in enumerate(pretrained_f.readlines()):\n",
    "                    if i == 0:  # 若第一行是标题， 则跳过 部分预训练模型第一行是词数和词嵌入\n",
    "                        continue\n",
    "                    lin = line.strip().split(' ')\n",
    "                    if lin[0] in word_to_id:\n",
    "                        idx = word_to_id[lin[0]]\n",
    "                        emb = [float(x) for x in lin[1: config.embedding_dim + 1]]\n",
    "                        embeddings[idx] = np.asarray(emb, dtype='float')\n",
    "                pretrained_f.close()\n",
    "                np.savez_compressed(vocab_tensor_path, embeddings=embeddings)  # emb\n",
    "                embedding_pretrained = embeddings.astype('float32')\n",
    "        return embedding_pretrained"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T10:55:33.027599200Z",
     "start_time": "2023-05-16T10:55:33.021615400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class config(object):\n",
    "    def __init__(self):\n",
    "        # 路径类 带*的是运行前的必要文件  未带*文件/文件夹若不存在则训练过程会生成\n",
    "        self.train_path = r'D:\\PycharmProjects\\nlp\\LessonStudy\\第五次课NER\\ner.train'  # *\n",
    "        self.dev_path = r'D:\\PycharmProjects\\nlp\\LessonStudy\\第五次课NER\\ner.dev'  # *\n",
    "        self.class_ls_path = r'D:\\PycharmProjects\\nlp\\LessonStudy\\第五次课NER\\ner.label'  # *\n",
    "        self.pretrain_dir = r'D:\\PycharmProjects\\nlp\\LessonStudy\\第五次课NER\\\\'  # 前期下载的预训练词向量*\n",
    "        self.test_path = r'D:\\PycharmProjects\\nlp\\LessonStudy\\第五次课NER\\ner.test'\n",
    "        self.vocab_path = 'vocab.pkl'\n",
    "        self.model_save_dir = 'checkpoint'\n",
    "        self.model_save_name = self.model_save_dir + '/BiLSTM_CRF.ckpt'  # 保存最佳dev acc模型\n",
    "\n",
    "        # 可调整的参数\n",
    "        # 搜狗新闻:embedding_SougouNews.npz, 腾讯:embedding_Tencent.npz,  若不存在则后期生成\n",
    "        # 随机初始化:random\n",
    "        self.embedding_type = 'embedding_SougouNews.npz'\n",
    "        self.use_gpu = True  # 是否使用gpu(有则加载 否则自动使用cpu)\n",
    "        self.batch_size = 128\n",
    "        self.num_epochs = 40  # 训练轮数\n",
    "        self.num_workers = 0  # 启用多线程\n",
    "        self.learning_rate = 0.001  # 训练发现0.001比0.01收敛快(Adam)\n",
    "        self.embedding_dim = 300  # 词嵌入维度\n",
    "        self.hidden_size = 300  # 隐藏层维度\n",
    "        self.num_layers = 2  # RNN层数\n",
    "        self.bidirectional = True  # 双向 or 单向\n",
    "        self.require_improvement = 1  # 1个epoch若在dev上acc未提升则自动结束\n",
    "\n",
    "        # 由前方参数决定  不用修改\n",
    "        self.class_ls = []\n",
    "        self.num_class = len(self.class_ls)\n",
    "        self.vocab_len = 0  # 词表大小(训练集总的字数(字符级)） 在embedding层作为参数 后期赋值\n",
    "        self.embedding_pretrained = None  # 根据config.embedding_type后期赋值  random:None  else:tensor from embedding_type\n",
    "        if self.use_gpu and torch.cuda.is_available():\n",
    "            self.device = 'cuda:0'\n",
    "        else:\n",
    "            self.device = 'cpu'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T10:55:34.655378600Z",
     "start_time": "2023-05-16T10:55:34.649347100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def build_dataset(config):\n",
    "    if os.path.exists(config.vocab_path):  # 加载词典\n",
    "        vocab = pickle.load(open(config.vocab_path, 'rb'))\n",
    "    else:\n",
    "        vocab = build_vocab(config.train_path, config.class_ls_path)  # 用训练数\n",
    "        with open(config.vocab_path, 'wb') as f:\n",
    "            pickle.dump(vocab, f)  # 存储每个字及对应索引的字典 eg: 我: 56 vocab[\n",
    "        config.vocab_len = len(vocab)\n",
    "        config.class_ls = [x.strip() for x in open(config.class_ls_path, 'r', encoding='utf-8').readlines()]\n",
    "        print(f'\\nVocab size: {len(vocab)}')\n",
    "\n",
    "    train_data = Mydataset(config.train_path, config, vocab)\n",
    "    dev_data = Mydataset(config.dev_path, config, vocab)\n",
    "    train_loader = DataLoader(dataset=train_data,\n",
    "                              batch_size=config.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=config.num_workers)\n",
    "    dev_loader = DataLoader(dataset=dev_data,\n",
    "                            batch_size=config.batch_size,\n",
    "                            shuffle=True,\n",
    "                            num_workers=config.num_workers)\n",
    "    if os.path.exists(config.test_path):\n",
    "        test_data = Mydataset(config.test_path, config, vocab)\n",
    "        test_loader = DataLoader(dataset=test_data,\n",
    "                                 batch_size=config.batch_size,\n",
    "                                 shuffle=False,\n",
    "                                 num_workers=config.num_workers)\n",
    "    else:  # 若无测试数据则加载验证集进行最终测试\n",
    "        test_loader = dev_loader\n",
    "    config.embedding_pretrained = torch.tensor(extract_vocab_tensor(config))\n",
    "    return train_loader, dev_loader, test_loader"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T10:55:35.530015100Z",
     "start_time": "2023-05-16T10:55:35.524030500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "67344it [00:00, 1080223.68it/s]\n",
      "11322it [00:00, 1031986.92it/s]\n",
      "11385it [00:00, 1141631.23it/s]\n"
     ]
    }
   ],
   "source": [
    "config = config()\n",
    "train_loader, dev_loader, test_loader = build_dataset(config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T10:55:36.416870900Z",
     "start_time": "2023-05-16T10:55:36.220585100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "next expected at least 1 argument, got 0",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: next expected at least 1 argument, got 0"
     ]
    }
   ],
   "source": [
    "next()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T10:55:39.020053100Z",
     "start_time": "2023-05-16T10:55:38.993098Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "START_TAG = 'START'\n",
    "STOP_TAG = 'STOP'\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Model, self).__init__()\n",
    "        if config.embedding_pretrained is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=False)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(config.vocab_len, config.embedding_dim)\n",
    "        if config.bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "        self.config = config\n",
    "        self.rnn = nn.LSTM(config.embedding_dim, config.hidden_size, config.num_layers, batch_first=True,\n",
    "                           bidirectional=config.bidirectional)\n",
    "\n",
    "        self.tag_ls = self.getTagLs(config)\n",
    "        self.tag2idx - self.getTagDic()\n",
    "        # 转换参数矩阵 输入i，j是得分从j转换到i\n",
    "        self.tagset_size = len(self.tag2idx)\n",
    "        self.crf = CRF(self.tagset_size)\n",
    "\n",
    "    def _get_lstm_features(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        h_0, c_0 = self._init_hidden(batchs=x.size(0))\n",
    "        out, (hidden, c) = self.rnn(x, (h_0, c_0))\n",
    "        out = self.hidden2tag(out)\n",
    "        out = out.transpose(0, 1)\n",
    "        return out\n",
    "\n",
    "    def neg_log_likelihood(self, x, tags):\n",
    "        tags = tags.unsqueeze(0)\n",
    "        feats = self._get_lstm_features(x)\n",
    "        return -self.crf(feats, tags)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_feats = self._get_lstm_features(x)\n",
    "        out = self.crf.decode(lstm_feats)\n",
    "        return out\n",
    "\n",
    "    # def _init_hidden(self, batchs):  # 初始化h_0和c_0 与GRU不同的是多了c_0(喜宝状)\n",
    "    #     h_0 = torch.zeros(self.config.num_layers*self.num_directions, batchs,)\n",
    "    #     c_0 = torch.zeros(self.config.num_layers*self.num_directions, batchs, s)\n",
    "    #     return self._make_tensor(h_0), self._make_tensor(c_0)\n",
    "\n",
    "    def _make_tensor(self, tensor):\n",
    "        tensor_ret = tensor.to(self.config.device)\n",
    "        return tensor_ret\n",
    "\n",
    "    def getTagLs(selfself, config):\n",
    "        tag_ls = config.class_ls\n",
    "        tag_ls.append(START_TAG)\n",
    "        tag_ls.append(STOP_TAG)\n",
    "        return tag_ls\n",
    "\n",
    "    def getTagDic(self):\n",
    "        tag_dic = {}\n",
    "        for idx, label in enumerate(self.tag_ls):\n",
    "            tag_dic[label] = idx\n",
    "        return tag_dic\n",
    "\n",
    "    def idx2Tag(self, idx):\n",
    "        return self.tag_ls[idx]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import time\n",
    "\n",
    "\n",
    "def train_test(config, model, train_loader, dev_loader, test_loader):\n",
    "    start = time.time()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "    create_dir_not_exists(config.model_save_dir)\n",
    "    if os.path.exists(config.model_save_name):\n",
    "        ckpt = torch.load(config.model_save_name)\n",
    "        model.load_state_dict(ckpt['optimizer'])\n",
    "        start_epoch = ckpt['epoch']\n",
    "        max_acc = ckpt['dev_acc']\n",
    "        best_epoch = start_epoch\n",
    "        print(f'Load epoch {start_epoch} successful...')\n",
    "    else:\n",
    "        start_epoch = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
