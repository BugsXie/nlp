{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QSOrhD8CYw7w",
    "ExecuteTime": {
     "end_time": "2023-05-10T08:20:10.882951700Z",
     "start_time": "2023-05-10T08:20:09.770074Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "VOQB9xS4YzbY"
   },
   "source": [
    "## Special tokens\n",
    "\n",
    "It is important that they don't appear in the actual vocab, hence this weird look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yGm72KVsYyFJ",
    "ExecuteTime": {
     "end_time": "2023-05-10T08:20:10.897912900Z",
     "start_time": "2023-05-10T08:20:10.884947400Z"
    }
   },
   "outputs": [],
   "source": [
    "PAD = \"@@PAD@@\"#补零\n",
    "UNK = \"@@UNK@@\"#填充"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbvIZ0uYZX43"
   },
   "source": [
    "## Hyperparameters \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dyKTbO9UaeHc",
    "ExecuteTime": {
     "end_time": "2023-05-10T08:20:10.920851200Z",
     "start_time": "2023-05-10T08:20:10.900904Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 20  # -1 for no truncation\n",
    "UNK_THRESHOLD = 5\n",
    "BATCH_SIZE = 128\n",
    "N_EPOCHS = 20\n",
    "LEARNING_RATE = 1e-3\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "N_RNN_LAYERS = 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Mb2hKVi3akuB"
   },
   "source": [
    "## Seeding Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_mWfC2m_akLQ",
    "ExecuteTime": {
     "end_time": "2023-05-10T08:20:10.930827700Z",
     "start_time": "2023-05-10T08:20:10.916863Z"
    }
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=1):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-e6EYB0Bauki"
   },
   "source": [
    "## Data Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_EvvrY9hXAj7",
    "ExecuteTime": {
     "end_time": "2023-05-10T08:20:10.957530200Z",
     "start_time": "2023-05-10T08:20:10.930827700Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def split_data(train_data, num_split=2000):\n",
    "    \"\"\"Splits the training data into training and development sets.\"\"\"\n",
    "    random.shuffle(train_data)\n",
    "    return train_data[:-num_split], train_data[-num_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LUYDWCH4MX7f",
    "ExecuteTime": {
     "end_time": "2023-05-10T08:20:11.523591800Z",
     "start_time": "2023-05-10T08:20:10.949039300Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_file = r\"D:\\PycharmProjects\\nlp\\data\\中文文本分类数据集\\data\\train.txt\"\n",
    "df = pd.read_csv(train_file, sep=\"\\t\", header=None, names=[\"id\", \"category\", \"sentence\"])\n",
    "\n",
    "rows = []\n",
    "for index, row in df[['id', 'category', 'sentence']].iterrows():\n",
    "    rows.append({\n",
    "            'sentence': row['sentence'],\n",
    "            'label': row['category'],\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xtXXoZA7Ncz4",
    "outputId": "c4015f3b-25b0-46c2-a502-2357be385843",
    "ExecuteTime": {
     "end_time": "2023-05-10T08:20:11.538555100Z",
     "start_time": "2023-05-10T08:20:11.523591800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'sentence': '(2)若伴便秘者符合罗马Ⅳ功能性便秘诊断标准，若伴夜尿症者符合夜尿症的诊断标准；', 'label': 'Diagnostic'}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "au59ApBKa8_r",
    "ExecuteTime": {
     "end_time": "2023-05-10T08:20:11.570982300Z",
     "start_time": "2023-05-10T08:20:11.539552600Z"
    }
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "def tokenize(data, max_seq_len=MAX_SEQ_LEN):\n",
    "    for example in data:\n",
    "        example['text']=[word for word in jieba.cut(example['sentence'])][:max_seq_len]\n",
    "    # \"\"\"\n",
    "    # Here we use nltk to tokenize data. There are many othe possibilities. We also truncate the\n",
    "    # sequences so that the training time and memory is more manageable. You can think of truncation\n",
    "    # as making a decision only looking at the first X words.\n",
    "    # \"\"\"\n",
    "      \n",
    "\n",
    "\n",
    "def create_vocab(data, unk_threshold=UNK_THRESHOLD):\n",
    "    counter=Counter(token for example in data for token in example['text'])\n",
    "    vocab = {token for token in counter if counter[token]>unk_threshold}\n",
    "    print(f'vocab size:{len(vocab)+2}')\n",
    "    print(f'Most common tokens:{counter.most_common(10)}')\n",
    "    token_to_idx={PAD:0,UNK:1}\n",
    "    for token in vocab:\n",
    "        token_to_idx[token]=len(token_to_idx)\n",
    "        return token_to_idx\n",
    "\n",
    "    # \"\"\"\n",
    "    # Creates a vocabulary with tokens that have frequency above unk_threshold and assigns each token\n",
    "    # a unique index, including the special tokens.\n",
    "    # \"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def apply_vocab(data, token_to_idx):\n",
    "    for example in data:\n",
    "        example['text']=[token_to_idx.get(token,token_to_idx[UNK])for token in example['text']]\n",
    "    # \"\"\"\n",
    "    # Applies the vocabulary to the data and maps the tokenized sentences to vocab indices as the\n",
    "    # model input.\n",
    "    # \"\"\"\n",
    "    \n",
    "\n",
    "def apply_label_map(data, label_to_idx):\n",
    "    for example in data:\n",
    "        example['label']=label_to_idx[example['label']]\n",
    "    # \"\"\"Converts string labels to indices.\"\"\"\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hryAitrnO0ZF",
    "outputId": "45d60041-217e-4b7d-ea74-a8f769515ef6",
    "ExecuteTime": {
     "end_time": "2023-05-10T08:20:12.810411300Z",
     "start_time": "2023-05-10T08:20:11.571979Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache D:\\LOCALT~1\\jieba.cache\n",
      "Loading model cost 0.581 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:1452\n",
      "Most common tokens:[('；', 3545), ('.', 2818), (' ', 2556), ('、', 2451), ('）', 2342), ('（', 2059), ('的', 1979), ('或', 1718), ('。', 1544), ('，', 1521)]\n"
     ]
    }
   ],
   "source": [
    "tokenize(rows)\n",
    "token_to_idx = create_vocab(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tt72-8qNPN3I",
    "outputId": "a679fab6-90af-4ba1-e464-94ad222f67ba",
    "ExecuteTime": {
     "end_time": "2023-05-10T08:20:12.854387700Z",
     "start_time": "2023-05-10T08:20:12.812404900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'@@PAD@@': 0, '@@UNK@@': 1, '循环': 2}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2tIx8lwzR5LB",
    "outputId": "81ffed60-e08e-4785-8a6c-2a07da7374a8",
    "ExecuteTime": {
     "end_time": "2023-05-10T08:20:12.860367500Z",
     "start_time": "2023-05-10T08:20:12.827366200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'sentence': '(2)若伴便秘者符合罗马Ⅳ功能性便秘诊断标准，若伴夜尿症者符合夜尿症的诊断标准；',\n 'label': 'Diagnostic',\n 'text': ['(',\n  '2',\n  ')',\n  '若伴',\n  '便秘',\n  '者',\n  '符合',\n  '罗马',\n  'Ⅳ',\n  '功能性',\n  '便秘',\n  '诊断',\n  '标准',\n  '，',\n  '若伴',\n  '夜尿症',\n  '者',\n  '符合',\n  '夜尿症',\n  '的']}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "hCjcKzSsbCFv",
    "ExecuteTime": {
     "end_time": "2023-05-10T08:20:12.860367500Z",
     "start_time": "2023-05-10T08:20:12.841331300Z"
    }
   },
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, data, pad_idx):\n",
    "        data = sorted(data, key=lambda example: len(example[\"text\"]))\n",
    "        self.texts = [example[\"text\"] for example in data]\n",
    "        self.labels = [example[\"label\"] for example in data]\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return [self.texts[index], self.labels[index]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        def tensorize(elements, dtype):\n",
    "            return [torch.tensor(element, dtype=dtype) for element in elements]\n",
    "             \n",
    "        def pad(tensors):\n",
    "            max_len = max(len(tensor) for tensor in tensors)\n",
    "            padded_tensors = [\n",
    "                F.pad(tensor, (0, max_len - len(tensor)), value=self.pad_idx)\n",
    "                for tensor in tensors\n",
    "            ]\n",
    "            return padded_tensors\n",
    "\n",
    "        texts, labels = zip(*batch)\n",
    "        return [\n",
    "            torch.stack(pad(tensorize(texts, torch.long)), dim=0),\n",
    "            torch.stack(tensorize(labels, torch.long), dim=0),\n",
    "        ]\n",
    "\n",
    "            \n",
    "\n",
    "         "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "mGH_3dXUbPhy"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "0zwMaK-5Wz31",
    "ExecuteTime": {
     "end_time": "2023-05-10T08:20:12.872336300Z",
     "start_time": "2023-05-10T08:20:12.856379Z"
    }
   },
   "outputs": [],
   "source": [
    "class SequenceClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_labels, n_rnn_layers, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.GRU(\n",
    "            embedding_dim, hidden_dim, num_layers=n_rnn_layers, batch_first=True, bidirectional=True\n",
    "        )\n",
    "        # We take the final hidden state at all GRU layers as the sequence representation.\n",
    "        # 2 because bidirectional.\n",
    "        layered_hidden_dim = hidden_dim * n_rnn_layers * 2\n",
    "        self.output = nn.Linear(layered_hidden_dim, n_labels)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text shape: (batch_size, max_seq_len) where max_seq_len is the max length\n",
    "        # len shape: (batch_size,)\n",
    "        non_padded_positions = text != self.pad_idx\n",
    "        lens = non_padded_positions.sum(dim=1)\n",
    "\n",
    "        # embedded shape: (batch_size, max_seq_len, embedding_dim)\n",
    "        embedded = self.embedding(text)\n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lens.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )#利用PyTorch提供的pack_padded_sequence()函数，将嵌入向量打包成压缩形式，便于GRU处理变长输入数据\n",
    "\n",
    "\n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        # shape: (batch_size, n_layers * n_directions * hidden_dim)\n",
    "        hidden = hidden.transpose(0, 1).reshape(hidden.shape[1], -1)\n",
    "\n",
    "        return self.output(hidden)\n",
    "\n",
    "    # def forward(self, text):\n",
    "    #     # text: [batch_size, seq_len]\n",
    "    #\n",
    "    #     # Masking for padding tokens.\n",
    "    #     # Create a binary mask of shape [batch_size, seq_len], where padding tokens have value 0 and others have value 1.\n",
    "    #     mask = (text != self.pad_idx).float()  # mask: [batch_size, seq_len]\n",
    "    #\n",
    "    #     # Word embedding layer.\n",
    "    #     embedded = self.embedding(text)  # embedded: [batch_size, seq_len, embedding_dim]\n",
    "    #\n",
    "    #     # Bidirectional GRU layer.\n",
    "    #     rnn_output, _ = self.rnn(embedded)  # rnn_output: [batch_size, seq_len, hidden_dim * 2]\n",
    "    #\n",
    "    #     # Apply the mask to the output of GRU.\n",
    "    #     # The output of padding tokens are masked out, so they have no effect on the following operations.\n",
    "    #     masked_output = rnn_output * mask.unsqueeze(-1)  # masked_output: [batch_size, seq_len, hidden_dim * 2]\n",
    "    #\n",
    "    #     # Sequence representation layer.\n",
    "    #     # We take the final hidden state at all GRU layers as the sequence representation.\n",
    "    #     # Concatenate the final hidden states from the forward and backward directions, then flatten them.\n",
    "    #     # This results in a tensor of shape [batch_size, hidden_dim * 2 * n_rnn_layers].\n",
    "    #     sequence_rep = torch.cat((\n",
    "    #         masked_output[:, -1, :self.rnn.hidden_size],\n",
    "    #         masked_output[:, 0, self.rnn.hidden_size:]\n",
    "    #     ), dim=-1).view(-1, self.rnn.hidden_size * 2 * self.rnn.num_layers)\n",
    "    #\n",
    "    #     # Output layer.\n",
    "    #     logits = self.output(sequence_rep)  # logits: [batch_size, n_labels]\n",
    "    #\n",
    "    #     return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZUQh0og3bvB6",
    "ExecuteTime": {
     "end_time": "2023-05-10T08:20:12.905248300Z",
     "start_time": "2023-05-10T08:20:12.873333500Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "pS0AmR1Cbwqj"
   },
   "source": [
    "## Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-XLLySqnbVGG",
    "ExecuteTime": {
     "end_time": "2023-05-10T08:20:12.918212500Z",
     "start_time": "2023-05-10T08:20:12.888294Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    for texts, labels in tqdm(dataloader):\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        output = model(texts)\n",
    "        loss = F.cross_entropy(output, labels)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    count = correct = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in tqdm(dataloader):\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            # shape: (batch_size, n_labels)\n",
    "            output = model(texts)\n",
    "            # shape: (batch_size,)\n",
    "            predicted = output.argmax(dim=-1)\n",
    "            count += len(predicted)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Auuracy:{correct / count}\")\n",
    "     \n",
    "             "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "9EJhpMerb7s4"
   },
   "source": [
    "## Running training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ynnOLpb0PcHr",
    "ExecuteTime": {
     "end_time": "2023-05-10T08:20:13.831056600Z",
     "start_time": "2023-05-10T08:20:12.904250800Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_file = r\"D:\\PycharmProjects\\nlp\\data\\中文文本分类数据集\\data\\train.txt\"\n",
    "df = pd.read_csv(train_file, sep=\"\\t\", header=None, names=[\"id\", \"category\", \"sentence\"])\n",
    "\n",
    "rows = []\n",
    "for index, row in df[['id', 'category', 'sentence']].iterrows():\n",
    "    rows.append({\n",
    "            'sentence': row['sentence'],\n",
    "            'label': row['category'],\n",
    "            })\n",
    "    \n",
    "\n",
    "train_data, test_data = split_data(rows, int(0.1*len(rows)))\n",
    "train_data, dev_data = split_data(train_data, int(0.1*len(train_data)))\n",
    "\n",
    "for data in (train_data, dev_data, test_data):\n",
    "    tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "shU-LlT7QB_e",
    "outputId": "f8d5f775-7545-4f02-8271-c69ba263b755",
    "ExecuteTime": {
     "end_time": "2023-05-10T08:20:13.891409400Z",
     "start_time": "2023-05-10T08:20:13.831056600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:1263\n",
      "Most common tokens:[('；', 2913), ('.', 2286), (' ', 2059), ('、', 1962), ('）', 1910), ('（', 1669), ('的', 1624), ('或', 1383), ('。', 1259), ('，', 1231)]\n"
     ]
    }
   ],
   "source": [
    "token_to_idx = create_vocab(train_data)\n",
    "\n",
    "label_to_idx = {\n",
    "    \"Addictive Behavior\": 0, \n",
    "    \"Age\": 1,\n",
    "    \"Allergy Intolerance\": 2,\n",
    "    \"Compliance with Protocol\": 3,\n",
    "    \"Consent\": 4,\n",
    "    \"Diagnostic\": 5,\n",
    "    \"Disease\": 6,\n",
    "    \"Enrollment in other studies\": 7,\n",
    "    \"Laboratory Examinations\": 8,\n",
    "    \"Life Expectancy\": 9,\n",
    "    \"Organ or Tissue Status\": 10,\n",
    "    \"Pharmaceutical Substance or Drug\": 11,\n",
    "    \"Risk Assessment\": 12,\n",
    "    \"Smoking Status\": 13,\n",
    "    \"Therapy or Surgery\": 14,\n",
    "}\n",
    "for data in (train_data, dev_data, test_data):\n",
    "    apply_vocab(data, token_to_idx)\n",
    "    apply_label_map(data, label_to_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "jh09sXi9Q0qh",
    "ExecuteTime": {
     "end_time": "2023-05-10T08:20:13.897394600Z",
     "start_time": "2023-05-10T08:20:13.867474800Z"
    }
   },
   "outputs": [],
   "source": [
    "pad_idx = token_to_idx[PAD]\n",
    "train_dataset = SentimentDataset(train_data, pad_idx)\n",
    "dev_dataset = SentimentDataset(dev_data, pad_idx)\n",
    "test_dataset = SentimentDataset(test_data, pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KvxSgi0PQ3hC",
    "outputId": "d3f8c52e-4818-4373-8f11-e0d7ec8f36b8",
    "ExecuteTime": {
     "end_time": "2023-05-10T08:20:13.898391700Z",
     "start_time": "2023-05-10T08:20:13.891409400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 1791375 parameters.\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=train_dataset.collate_fn\n",
    ")\n",
    "dev_dataloader = DataLoader(\n",
    "    dev_dataset, batch_size=BATCH_SIZE, collate_fn=dev_dataset.collate_fn\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, collate_fn=test_dataset.collate_fn\n",
    ")\n",
    "\n",
    "model = SequenceClassifier(\n",
    "    len(token_to_idx), EMBEDDING_DIM, HIDDEN_DIM, len(label_to_idx), N_RNN_LAYERS, pad_idx\n",
    ")\n",
    "print(f\"Model has {count_parameters(model)} parameters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oexuldjqQ8IE",
    "outputId": "9104faed-aec3-4522-fef0-4c5bdcad0edb",
    "ExecuteTime": {
     "end_time": "2023-05-10T08:20:38.104253200Z",
     "start_time": "2023-05-10T08:20:13.893405800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random baseline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 33.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auuracy:0.02361111111111111\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 43.47it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 176.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auuracy:0.15555555555555556\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 47.29it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 171.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auuracy:0.15138888888888888\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 47.82it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 182.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auuracy:0.13194444444444445\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 48.21it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 176.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auuracy:0.1527777777777778\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 47.25it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 174.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auuracy:0.15833333333333333\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 47.18it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 179.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auuracy:0.15555555555555556\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 48.39it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 174.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auuracy:0.14166666666666666\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 46.95it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 174.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auuracy:0.15\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 48.00it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 160.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auuracy:0.17083333333333334\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 48.36it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 162.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auuracy:0.14722222222222223\n",
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 48.30it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 174.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auuracy:0.15833333333333333\n",
      "Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 47.86it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 160.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auuracy:0.16111111111111112\n",
      "Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 48.00it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 169.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auuracy:0.15416666666666667\n",
      "Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 48.16it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 174.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auuracy:0.16805555555555557\n",
      "Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 47.80it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 174.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auuracy:0.15694444444444444\n",
      "Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 48.21it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 169.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auuracy:0.15555555555555556\n",
      "Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 48.30it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 164.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auuracy:0.15138888888888888\n",
      "Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 47.56it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 174.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auuracy:0.1597222222222222\n",
      "Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 47.92it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 179.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auuracy:0.16944444444444445\n",
      "Epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:01<00:00, 47.50it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 179.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auuracy:0.15833333333333333\n",
      "Test set performance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 171.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auuracy:0.19125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Random baseline\")\n",
    "evaluate(model, dev_dataloader, device)\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}\")  # 0-based -> 1-based\n",
    "    train(model, train_dataloader, optimizer, device)\n",
    "    evaluate(model, dev_dataloader, device)\n",
    "print(f\"Test set performance\")\n",
    "evaluate(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "bYq0HEUcRHvm",
    "ExecuteTime": {
     "end_time": "2023-05-10T08:20:38.104253200Z",
     "start_time": "2023-05-10T08:20:38.098760Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
